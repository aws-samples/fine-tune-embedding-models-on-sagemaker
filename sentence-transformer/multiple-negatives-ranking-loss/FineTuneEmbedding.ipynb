{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcba6e2",
   "metadata": {},
"source": [
    "# Improve RAG Accuracy with Finetuned Embedding Models on Amazon SageMaker\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Install Dependencies](#install-dependencies)\n",
    "3. [Load Data and Train the Model](#load-data-and-train-the-model)\n",
    "4. [Create Inference Script](#create-inference-script)\n",
    "5. [Upload the Model](#upload-the-model)\n",
    "6. [Deploy Model on SageMaker](#deploy-model-on-sagemaker)\n",
    "7. [Invoke the Model](#invoke-the-model)\n",
    "8. [Compare Predictions](#compare-predictions)\n",
    "\n",
    "<a id='introduction'></a>\n",
    "### Introduction\n",
    "\n",
    "This notebook demonstrates how to use Amazon SageMaker to fine-tune a Sentence Transformer embedding model and deploy it with Amazon SageMaker Endpoint. For more information about Finetuning Sentence Transformer, see [Sentence Transformer Training Overview](https://www.sbert.net/docs/sentence_transformer/training_overview.html).\n",
    "\n",
    "We will fine-tune the embedding model [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). It is an open-source sentence-transformers model fine-tuned on a 1B sentence pairs dataset. It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "To fine-tune it, we will use the [Bedrock FAQ](https://aws.amazon.com/bedrock/faqs/), a dataset of questions and answer pairs, using the [Multiple Negatives Ranking Loss function](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss).\n",
    "\n",
    "<a id='install-dependencies'></a>\n",
    "### Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf626f15-ce6f-4827-b6d6-781ad526774e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pathos==0.3.2\n",
    "!pip install datasets==2.19.2\n",
    "!pip install transformers==4.40.2\n",
    "!pip install transformers[torch]==4.40.2\n",
    "!pip install sentence_transformers==3.1.1\n",
    "!pip install accelerate==1.0.0\n",
    "!pip install sagemaker==2.224.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2fa78",
   "metadata": {},
   "source": [
    "<a id='load-data-and-train-the-model'></a>\n",
    "## Load Data and Train the Model\n",
    "\n",
    "The following code snippet demonstrates how to load a training dataset from a JSON file, prepare the data for training, and then fine-tune the pre-trained model. After fine-tuning, the updated model is saved.\n",
    "\n",
    "The `EPOCHS` variable determines the number of times the model will iterate over the entire training dataset during the fine-tuning process. A higher number of epochs typically leads to better convergence and potentially improved performance, but may also increase the risk of overfitting if not properly regularized.\n",
    "\n",
    "In this example, we have a small training set consisting of only 100 records. As a result, we are using a high value for the `EPOCHS` parameter. Typically, in real-world scenarios, you would have a much larger training set. In such cases, the `EPOCHS` value should be a single or two-digit number to avoid overfitting the model to the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1f8ae-23ad-4854-94ce-2d4d5a3e3f6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "import json\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "dataset = load_data(\"training.json\")\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Convert the dataset to the required format\n",
    "train_examples = [InputExample(texts=[data[\"sentence1\"], data[\"sentence2\"]]) for data in dataset]\n",
    "\n",
    "# Create a DataLoader object\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "\n",
    "# Define the loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "EPOCHS=100\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"opt/ml/model/\",safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d874e8",
   "metadata": {},
   "source": [
    "<a id='create-inference-script'></a>\n",
    "## Create inference.py File\n",
    "\n",
    "To deploy and serve the fine-tuned embedding model for inference, we create an `inference.py` Python script that serves as the entry point. This script implements two essential functions: `model_fn` and `predict_fn`, as required by AWS SageMaker for deploying and using machine learning models.\n",
    "\n",
    "The `model_fn` is responsible for loading the fine-tuned embedding model and the associated tokenizer. On the other hand, the `predict_fn` takes input sentences, tokenizes them using the loaded tokenizer, and computes their sentence embeddings using the fine-tuned model. To obtain a single vector representation for each sentence, it performs mean pooling over the token embeddings, followed by normalization of the resulting embedding. Finally, the `predict_fn` returns the normalized embeddings as a list, which can be further processed or stored as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251ce77-43d8-4a9c-9315-c65e3b53c1d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile opt/ml/model/inference.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def model_fn(model_dir, context=None):\n",
    "  # Load model from HuggingFace Hub\n",
    "  tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}/model\")\n",
    "  model = AutoModel.from_pretrained(f\"{model_dir}/model\")\n",
    "  return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer, context=None):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    sentences = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # return dictonary, which will be json serializable\n",
    "    return {\"vectors\": sentence_embeddings[0].tolist()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875a2cc",
   "metadata": {},
   "source": [
    "<a id='upload-the-model'></a>\n",
    "## Upload the Model\n",
    "\n",
    "After creating the `inference.py` script, we package it together with the fine-tuned embedding model into a single `model.tar.gz` file. This compressed file can then be uploaded to an Amazon S3 bucket, making it accessible for deployment as a SageMaker endpoint."

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356331c8-2637-4d07-bad4-bb1e8acf88ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "model_dir = \"opt/ml/model\"\n",
    "model_tar_path = \"model.tar.gz\"\n",
    "\n",
    "with tarfile.open(model_tar_path, \"w:gz\") as tar:\n",
    "    tar.add(model_dir, arcname=os.path.basename(model_dir))\n",
    "    \n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Get the region name\n",
    "session = boto3.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get the account ID from STS (Security Token Service)\n",
    "sts_client = session.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "model_path = f\"s3://sagemaker-{region_name}-{account_id}/model_trained_embedding/model.tar.gz\"\n",
    "\n",
    "bucket_name = f\"sagemaker-{region_name}-{account_id}\"\n",
    "s3_key = \"model_trained_embedding/model.tar.gz\"\n",
    "\n",
    "with open(model_tar_path, \"rb\") as f:\n",
    "    s3.upload_fileobj(f, bucket_name, s3_key)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd12ec",
   "metadata": {},
   "source": [
    "<a id='deploy-model-on-sagemaker'></a>\n",
    "## Deploy Model on SageMaker\n",
    "\n",
    "Finally, we can deploy our fine-tuned model on a SageMaker Endpoint using `SageMaker HuggingFaceModel`."

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9d0d1-60d8-4725-84e3-afbcfded5a88",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_path,  # path to your trained SageMaker model\n",
    "   role=sagemaker.get_execution_role(),                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.26\",                           # Transformers version used\n",
    "   pytorch_version=\"1.13\",                                # PyTorch version used\n",
    "   py_version='py39',                                    # Python version used\n",
    "   entry_point=\"opt/ml/model/inference.py\",\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe3822",
   "metadata": {},
   "source": [
    "<a id='invoke-the-model'></a>\n",
    "## Invoke the Model\n",
    "\n",
    "You can invoke the model using the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8c26f-beae-416d-8233-0bda0ebca4ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example request: you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Are Agents fully managed?.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e4d85",
   "metadata": {},
   "source": [
    "<a id='compare-predictions'></a>\n",
    "## Compare Predictions\n",
    "\n",
    "To illustrate the impact of fine-tuning, we can compare the cosine similarity scores between two semantically related sentences using both the original pre-trained model and the fine-tuned model. A higher cosine similarity score indicates that the two sentences are more semantically similar, as their embeddings are closer in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221624b7-acdd-4c31-b7bb-69d41e51c35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "pretrained_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"What are Agents, and how can they be used?\"\n",
    "    , \n",
    "    \"Agents for Amazon Bedrock are fully managed capabilities that automatically break down tasks, create an orchestration plan, securely connect to company data through APIs, and generate accurate responses for complex tasks like automating inventory management or processing insurance claims.\"\n",
    "]\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_x= pretrained_model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_y = pretrained_model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_x, embedding_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be0ba2-7647-4e94-b323-1a059f1b1280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "data1 = {\n",
    "   \"inputs\": \n",
    "    \"What are Agents, and how can they be used?\"\n",
    "}\n",
    "\n",
    "data2 = {\n",
    "   \"inputs\": \n",
    "    \"Agents for Amazon Bedrock are fully managed capabilities that automatically break down tasks, create an orchestration plan, securely connect to company data through APIs, and generate accurate responses for complex tasks like automating inventory management or processing insurance claims.\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "el1 = predictor.predict(data1)\n",
    "el2 = predictor.predict(data2)\n",
    "\n",
    "util.pytorch_cos_sim(el1[\"vectors\"], el2[\"vectors\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b4e61-13a0-46e6-beec-5677ba48158a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
